---
title: 生成AI入門してみた① 基本編
tags:
  - 初心者
  - 生成AI
  - LLM
private: false
updated_at: '2026-01-01T16:33:49+09:00'
id: 44da29e6d73ff8c84b6d
organization_url_name: null
slide: false
ignorePublish: false
---

# はじめに

私は現在 Web エンジニア（インフラ・バックエンド寄り）として働いており、毎日業務で ChatGPT や Cusor, Codex など生成 AI・コーディングエージェントを使っています。

ただ、AI を「利用する側」に留まっており、AI の仕組みを理解しておらず、「ずっとこのままで良いのか」と疑問を持つようになりました。

**仕組みを理解することで、もっと AI を使いこなすこともできるし、今後 AI を「作る側」に回るチャンスも掴みやすくなる気がします。**

ちょうど年末年始で時間があったこともあり、生成 AI の「裏側の仕組み」について勉強したので、この記事にまとめてみようと思います。

どなたかの参考になると嬉しいです。

# 参考

今回、こちらの本を読んで勉強させていただきました。
私は数学がどうしても苦手で、難しい数式やアルゴリズムが出てくると思考停止してしまうのですが、こちらの本ではそんなアレルギーを発症せずに興味深く読み進めることができました。

『生成 AI「思考」の裏側 なぜ賢いのか？ なぜ間違うのか？』

https://bookplus.nikkei.com/atcl/catalog/25/10/09/02257/

# 記事の構成

記事は 4 つくらいに分けて、まとめようと思っています。
この記事では第 1 部のみ書き、後は後続の記事に分けます。

### 第 1 部: 基礎編

キーワード

- 推論・学習
- LLM
- ニューラルネットワーク・ディープラーニング
- LLM と AI エージェントの違い

### 第 2 部: 自然言語処理の仕組み編

キーワード

- 自然言語処理とは
- 機械翻訳との関係性
- 単語の意味を理解する仕組み
  - 単語を数値で表す必要性（ベクトル）
  - word2vec
- 文章を理解する仕組み
  - RNN
  - エンコーダ・デコーダ
  - seq2seq
  - Attention

### 第 3 部: Trasformer の仕組み編

キーワード

- Transformer の理解がなぜ重要か
- seq2seq と Transformer の違い
- Transformer の構造
- Transformer の学習の仕組み

### 第 4 部: 発展編

キーワード

- MoE
- RAG
- ReAct

# 第 1 部: 基礎編

## この章でわかること

ChatGPT などの生成 AI は、まるで人間のように「考えて」文章を書いているように見えます。

しかし実際には、中で起きているのは**非常に大量で高速な「計算」** です。

この章では、

- 「推論」という言葉の正体
- LLM（大規模言語モデル）とは何か
- 学習・ニューラルネットワーク・ディープラーニングの関係
- LLM と AI エージェントの違い

といった、生成 AI に関する基礎的な用語をを整理しました。

## 1-1. 「推論」という言葉には 2 つの意味がある

生成 AI の説明でよく「推論」というワードが出てきますが、実は「推論」は **2 つの異なる意味** で使われています。

日本語では同じ「推論」ですが、英語では別の単語です。
ここは私も理解しておらず、最初の落とし穴でした。

### reasoning の推論（人間っぽい思考）

こちらは、

- 前提や条件から結論を導く
- 目的を達成するために計画を立てる
- 状況に応じて方針を修正する

といった、**人間の思考に近い**推論です。

AI エージェントが「目標を立てて → 行動して → 結果を見て → 修正する」
といった振る舞いをするときに重要になるのが、この reasoning の推論です。

### inference の推論（モデルを動かす計算）

一方、LLM の文脈でよく言われる推論の多くは、こちらです。

1. 学習済みのモデルに入力を与え
2. 出力を計算する

という **純粋な計算処理** を指します。

ChatGPT にプロンプトを入力すると、「次にどの単語が来る確率が高いか」を計算しながら文章を生成します。
この処理が inference の推論です。

**AI 文脈での「学習」に対する「推論」でいうと、この inference の推論を指します。**

## 1-2. LLM とは？「大規模」とは何が大きいのか？

### LLM とは

**LLM は Large Language Model（大規模言語モデル）** の略です。

ChatGPT、Claude、Gemini などの対話型 AI の中核には、この LLM が使われています。

「言語モデル」とは、人間の言葉（自然言語）を扱うモデルのことです。
画像認識や音声認識とは別の種類の AI になります。

### 「大規模」とは何が大きいのか

意外と一言で何が「大規模」か、言い表すのが難しいようです。
単純にモデル自体が大きいとかそういうだけの意味ではなく、次のような要素が **総合的に大きい** ことを指すようです。

**1. パラメータ数**
モデル内部にある調整用の数値の数。
多いほど複雑な表現が可能になります。

**2. 学習データの規模**
数兆トークン規模のテキストを、多言語・多分野で学習します。

**3. 学習計算量（compute）**
学習には莫大な計算量が必要で、巨大な GPU クラスタが使われます。

**4. 学習・運用インフラの複雑さ**
並列化、分散処理、推論最適化など、モデルを動かすためのインフラにも工夫が必要です。

また、このような「規模」とともに、学習データの **「質」** も、 LLM の賢さの大きな要因となります。

## 1-3. 学習とは

ここまでで、LLM が「非常に大規模なモデル」であることを見てきました。

では、その巨大なモデルは**どうやって賢くなっているのか？**
次に、AI の「学習」が何をしているのかを見ていきます。

### ざっくり

たとえるなら、学習とは「大量の模試を解きながら、解き方のクセを少しずつ修正していく」ようなものです。（教師あり学習のイメージ）

1 回の模試で賢くなるわけではなく、間違えたポイントをもとに、解き方（＝パラメータ）を微調整し続けます。

### 学習とは「パラメータ調整」

ニューラルネットワークの中には、パラメータ（重み） と呼ばれる数値が大量にあります。

学習とは、

- 入力を与えて出力を計算し、
- 正解と比べてズレ（**損失**）を測り、
- そのズレが小さくなるようにパラメータを少しずつ調整する

という作業の繰り返しです。

これを大量のデータで行うことで、損失が最小となるように「重み」と「バイアス」というパラメータを調整します。
この作業のことを**最適化**といいます。

### データセットとラベル

学習では、

- 入力データ
- **正解データ（ラベル）**

をセットにした **データセット** が使われます。

たとえば画像認識なら、

- 画像：入力
- 「犬」：ラベル

という組み合わせです。

## 1-4. ニューラルネットワークとディープラーニング

LLM の正体は、ざっくり「巨大なニューラルネットワーク」であると言えます。

そのためここからは、ニューラルネットワークについて書いてみます。

### ニューラルネットワークとは

ニューラルネットワークは、「人間の脳の仕組みにヒントを得た情報処理モデル」です。

特徴的なのは、入力から出力までを 一気に判断するのではなく、段階的に処理する という点です。
この段階のことを **「層（レイヤー）」** と呼びます。

ニューラルネットワークは、主に次の 3 種類の層から構成されています。

- 入力層: データをそのまま受け取る層（文章、画像、数値など）
- **隠れ層（中間層）**: 入力を加工・変換しながら考える層 ※ 実際の「賢さ」はここに詰まっています
- 出力層: 最終的な判断結果を出す層（次の単語の確率、分類結果など）

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/577028/1c3ebc51-3647-480b-bab1-15abebf9f7d0.png)

[出典](https://www.skygroup.jp/media/article/3471/)

### ディープラーニングとは

ニューラルネットワークは LLM のベースですが、LLM がここまで賢くなれるのはディープラーニングという仕組みによるものです。

ディープラーニングは、「ニューラルネットワークのうち **隠れ層を複数重ねたもの**」を使った**学習手法**です。

層が深くなることで、単純な特徴から抽象的な特徴まで段階的に学習できるようになります。

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/577028/db32e5df-a796-489f-bba1-c94bf6bb7550.png)

#### 少しくわしく、隠れ層が増えると何が起きるのか

隠れ層が 1 つだけの場合、モデルが持てる「見方」は限られます。

一方で、隠れ層を何層も重ねると、

- 単純な特徴を組み合わせ
- それをさらに組み合わせ
- より抽象的な判断ができる

ようになります。

このように、 **多層（＝深い）構造のニューラルネットワークを使った学習手法がディープラーニング** です。

また補足として、**どんな特徴を使うかは、人間が決めず、モデル自身が学習の中で自動的に決めます**。

## 1-5. LLM と AI エージェントの違い

ここまで説明してきた LLM は、基本的に 「入力 → 出力」 を行うモデルです。

### LLM の特徴

- 与えられたプロンプトに対して
- 次の単語を確率的に予測し
- 文章を生成する

自分で目的を立てたり、行動を選んだりはしません。

### AI エージェントとは

AI エージェントは、

- 目標を設定し
- 計画を立て
- 行動し
- 結果を見て修正する

という一連の流れを、自律的に繰り返す仕組みです。

このとき、

- 頭脳として LLM （**reasoning の推論**）
- 行動としてツール実行

を組み合わせます。

つまり、ものすごくざっくりいうと

- LLM は **「頭」**
- AI エージェントは **「頭＋手足」**

というイメージになるのかなと思います。

※ AI エージェントの仕組みについては、主に第 4 章で触れていく予定です。

### おまけ：なぜ GPU が生成 AI で有利なのか

生成 AI の処理は、同じ計算を大量のデータに対して**同時に行う**という特徴があります。

GPU は、大量の単純な計算を並列に処理するのが得意です。

特に、後述の章で出てくる Transformer は、単語同士の関係を**一括で**計算する構造を持っており、GPU との相性が非常に良いです。

# まとめ

この章では、生成 AI の土台となる考え方を整理しました。

次の章では、「そもそもコンピュータは言葉をどう扱っているのか？」という疑問に答えるため、自然言語処理の仕組みをまとめようと思います。
