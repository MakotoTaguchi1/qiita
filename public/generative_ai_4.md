---
title: 生成 AI 入門してみた ④ 発展編
tags:
  - 初心者
  - React
  - rag
  - 生成AI
private: false
updated_at: '2026-01-03T14:28:31+09:00'
id: e2d229b2f11ba8fadaef
organization_url_name: null
slide: false
ignorePublish: false
---

## はじめに

ここまでの第 1〜3 部では、

- LLM が何者なのか
- 言葉をどう数値として扱っているのか
- LLM の中核である Transformer の仕組み

といった、「生成 AI の中身」にフォーカスしてきました。

この第 4 部では少し視点を変えて、**「LLM をさらに進化させ、有効活用するための技術」**という発展的な仕組みを見ていきます。

今回取り上げるのは次の 3 つです。

- **MoE（Mixture of Experts）**
- **RAG（Retrieval Augmented Generation）**
- **ReAct（Reasoning and Acting）**

※ といっても他にも色々あるのですが、今回は個人的に気になった 3 つに絞りました。

## 4-1. MoE（Mixture of Experts）

〜 全部を使わず、賢く使う 〜

LLM は大きいほど賢くなりやすい一方で、学習も推論もコストが跳ね上がります。  
**「全部の知識を毎回フル稼働させる必要ある？」**という疑問から生まれたのが **MoE** です。

MoE（Mixture of Experts）は、ネットワークを複数の「専門家」に分け、入力に応じて**使う部分だけ選ぶ**仕組みです。

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/577028/7ecaf0a2-e520-477f-84ef-8a7d6c8d0893.png)

[出典](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

Transformer では、**Feed Forward（次の単語を決めるための計算）部分**に使われることが多いです。  
巨大な 1 個のネットワークではなく、**小さなネットワークを複数用意して必要なものだけ動かす**イメージです。

その結果、

- パラメータ総数は大きいまま
- 実際に動く計算は小さくできる

という、いいとこ取りが可能になります。

人間に例えると、以下のような感じです

```
すべての質問に、全社員が会議に参加する会社
→
内容に応じて、関係ある人・有識者だけが集まる会社
```

LLM を **「巨大化させ続ける」ための現実的な工夫** が MoE です。

## 4-2. RAG（Retrieval Augmented Generation）

〜 覚えていないことは、調べてから答える 〜

### RAG の概要

LLM は「次に来る単語を確率で選ぶ」仕組みなので、**学習していない事実・最新情報・社内データ**に弱いです。  
そこで **RAG** は「知らないことは外部から調べてから答える」をやります。

### RAG の流れ

RAG の流れは、ざっくりこうです。

1. ユーザーが質問する
2. その質問文をもとに、外部データベースを検索する
3. 関連しそうな情報をいくつか取得する
4. それらの情報を **プロンプトとして LLM に渡す**
5. LLM が、それを踏まえて回答を生成する

つまり、**「調べ物をしてから文章を書く」**という、人間にかなり近い動きです。

### ベクトル検索

RAG ではデータベースに問い合わせるとき、通常の検索ではなく **ベクトル検索** がよく使われます。  
キーワード一致だけでなく、「意味が近いかどうか」で探せるので、**言い回しは違うけど内容が近い情報**を拾いやすいです。
（第 2 部の埋め込みが活躍します）

### 外部検索ツールとの違い

**Tavily などの外部検索ツールとの違い**も整理しておきます。

RAG は「検索 → 取得 → プロンプトに入れる」までを含んだ**仕組み（設計）**の名前です。  
一方で Tavily は「Web から情報を取ってくる**道具**」に近い存在です。  
実運用では **RAG は社内・内部 DB を対象にするケースが多く**、**Tavily 等のツール は Web 検索等で外部情報を検索したいときに使う**というように使い分けられることがよくあります。

なので、

- **RAG = どう調べて、どう使うかの考え方**
- **Tavily = 調べるための外部検索ツール**

という関係です。  
ただし設計としては、**RAG は内部 DB に限定されず Web 検索も組み込める**ので、例えば「Tavily などのツールを使った RAG」もありえます。

### RAG が向いている場面

RAG が向いているのは、例えばこんな場面です。

- 社内ドキュメント検索
- 製品マニュアル Q&A
- 最新情報を扱うチャットボット
- 自分専用のナレッジ AI

「LLM を賢くする」というより、**「LLM を安全に・実用的に使う」ための仕組み**と言えるかもしれません。

## 4-3. ReAct

〜 考えながら、行動する AI 〜

LLM は基本的に「頭脳」だけで、自分で行動できません。  
そこで **AI エージェント** という考え方が出てきます。

**ReAct（Reasoning and Acting）** は、**「考える」と「行動する」を交互に回す**設計思想です。

ざっくり流れはこうです。

1. 目的を確認する
2. 次にやるべきことを考える（**Reasoning**）
3. 必要なツールを使う（**Acting**）
4. 結果を受け取り、再度考える
5. 目的が達成されるまで繰り返す

例えば、

- Web 検索
- API 呼び出し
- ファイル操作

などを、LLM が「考えながら」使います。

ReAct によって、単なる文章生成・単発の質問応答から一歩進んで、
**「目的達成のために動く AI」**が実現できるようになります。

最近よく聞く

- AI エージェント
- コーディングエージェント

といったものの多くは、
この ReAct の考え方をベースに作られています。

## まとめ：第 4 部を通して

重要なのは、今回紹介した仕組みが**Transformer を置き換えるものではなく**、
Transformer という「頭脳」を土台にしながら、

- どうスケールさせるか
- どう現実世界と接続するか
- どう自律的に使うか

という発展的・補完的な仕組みを提供している点です。

## 最後に

LLM について、これまで中身を理解することは正直敬遠していましたが、今回仕組みを文章に起こしたり図を書いたりしてみて、だいぶ理解が進んだ気がします。

個人的にはこういった形で準備をしつつ、今後 LLM を利用したサービスの開発であったり、「作る側」に回るチャンスを掴めることができれば良いなと思います。

ここまで読んでいただき、ありがとうございました。
読まれた方が、生成 AI の裏側を少しでも立体的にイメージできるようになっていれば嬉しいです。
